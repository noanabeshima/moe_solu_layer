# SoLU MoE layer

Below is the result from using the SoLU MoE layer as an autoencoder for a sparse feature toy model fairly similar to https://transformer-circuits.pub/2022/toy_model/index.html:

![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNoa%2FUlTqeEfafC.png?alt=media&token=405747dc-2681-49aa-9228-1a472bfa5365)

![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNoa%2F1r11MQ9J_h.png?alt=media&token=1bb80203-e508-4ce9-ae31-a1109a3a6ff1)

![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FNoa%2FNteTIQ08bn.png?alt=media&token=1915a79b-4963-40fb-a1ff-073c2c1de666)

Note the features stored in negative solu neuron activations.

For more info, see these papers:
- https://transformer-circuits.pub/2022/toy_model/index.html
- https://transformer-circuits.pub/2022/solu/index.html
- https://arxiv.org/abs/2101.03961
